# OLLAMA [<img src="https://ollama.com/public/ollama.png" height="24px" style="background-color:white;">](https://ollama.com/)
Ollama allows you to run large language models locally for your personal projects. Ollama can be run [directly](https://github.com/ollama/ollama/blob/main/README.md), or through [docker](https://hub.docker.com/r/ollama/ollama). Ollama supports usage of both cpu and gpu. Consult your guides on how to enable gpu on docker containers for your prefered OS. Just running a llm server likely will not be enough. You may choose to access your server via a [client](https://github.com/ollama/ollama-python).